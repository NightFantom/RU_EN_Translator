{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctQPAr0no1xf"
   },
   "source": [
    "# Local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:15:46.833480Z",
     "start_time": "2019-04-08T06:15:46.830688Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iDsSIlnZo1xk"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXOcOLjMo1xu"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:09.672704Z",
     "start_time": "2019-04-08T06:20:09.668564Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pcgNjuwNo1xv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from translator_constants.global_constant import *\n",
    "from text_utils.vocabulary import Vocabulary\n",
    "from text_utils.fast_text import FastTextWrapper\n",
    "from tokenizer.word_punct_tokenizer import tokenizer_factory\n",
    "from dataset.ru_en_dataset import RUENDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:01.172572Z",
     "start_time": "2019-04-08T06:16:01.169214Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YP_O3a6jo1x3"
   },
   "outputs": [],
   "source": [
    "TENSORBOARD_LOG = os.path.join(BASE_PATH, \"tensorboard_log\")\n",
    "\n",
    "if not os.path.exists(TENSORBOARD_LOG):\n",
    "    os.makedirs(TENSORBOARD_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:02.759595Z",
     "start_time": "2019-04-08T06:16:02.755747Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UnHIGW5Ho1yD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Chosen {device.type}:{device.index} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:28.919379Z",
     "start_time": "2019-04-08T06:16:28.913735Z"
    },
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "yNTeqUT8o1yN"
   },
   "outputs": [],
   "source": [
    "def del_tensorboard_log():\n",
    "    import shutil\n",
    "    shutil.rmtree(TENSORBOARD_LOG)\n",
    "\n",
    "def load_data(word_dir:str)-> pd.DataFrame:\n",
    "    \n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.en\")\n",
    "    data_en = load_corpus(path)\n",
    "\n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.ru\")\n",
    "    data_ru = load_corpus(path)\n",
    "    \n",
    "    df = pd.DataFrame({RU_LABEL: data_ru, EN_LABEL: data_en})\n",
    "    return df\n",
    "\n",
    "def load_corpus(path:str)->list:\n",
    "    with open(path, mode=\"r\") as file:\n",
    "        data = file.readlines()\n",
    "    data = [s.strip().lower() for s in data]\n",
    "    return data\n",
    "\n",
    "def show_translation(ru_list, en_list):\n",
    "    index = random.randint(0, len(ru_list)-1)\n",
    "    ru_sent = \" \".join(ru_list[index])\n",
    "    en_sent = \" \".join(en_list[index])\n",
    "    print(ru_sent)\n",
    "    print(en_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb9Fo7RNo1yx"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:34.085505Z",
     "start_time": "2019-04-08T06:16:30.296805Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UEHlOaUyo1yz"
   },
   "outputs": [],
   "source": [
    "corpus_df = load_data(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:34.174186Z",
     "start_time": "2019-04-08T06:16:34.158852Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7Dlm4RL9o1y5"
   },
   "outputs": [],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:37.500359Z",
     "start_time": "2019-04-08T06:16:37.497006Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2eqOrSuLo1y-"
   },
   "outputs": [],
   "source": [
    "corpus_df = corpus_df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkjsAbu3o1zC"
   },
   "source": [
    "# Convert English tokens in one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:39.761083Z",
     "start_time": "2019-04-08T06:16:39.757856Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BVAnKBZto1zD"
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_factory(WORD_PUNCT_TOKENIZER_WITH_SOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:41.945388Z",
     "start_time": "2019-04-08T06:16:41.693193Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PXxOvbtdo1zG"
   },
   "outputs": [],
   "source": [
    "english_vocab = Vocabulary()\n",
    "english_tokens = corpus_df.apply(lambda x: tokenizer(x[EN_LABEL]), axis=1)\n",
    "english_vocab.fit(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:43.577057Z",
     "start_time": "2019-04-08T06:16:43.560890Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8b5glCO1o1zP"
   },
   "outputs": [],
   "source": [
    "en_encoder = LabelBinarizer(sparse_output=False)\n",
    "en_encoder.fit(range(english_vocab.max_index+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:44.621545Z",
     "start_time": "2019-04-08T06:16:44.561961Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xm8oYasao1zT"
   },
   "outputs": [],
   "source": [
    "en_sentence_list = english_vocab.transform(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:45.174201Z",
     "start_time": "2019-04-08T06:16:45.169830Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QR6SX-j_o1zY"
   },
   "outputs": [],
   "source": [
    "english_vocab.max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:16:45.962029Z",
     "start_time": "2019-04-08T06:16:45.957001Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WjdX36q5o1zc"
   },
   "outputs": [],
   "source": [
    "max_lenth = np.max([len(x) for x in en_sentence_list])\n",
    "max_lenth += 1\n",
    "print(f\"Max sequence lenth is {max_lenth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "floKubobo1zj"
   },
   "source": [
    "# Length histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "k0pQjgzLo1zm"
   },
   "outputs": [],
   "source": [
    "hist_data = np.histogram( [len(x) for x in sentence_list], bins=max_lenth)\n",
    "\n",
    "plt.bar(range(max_lenth), hist_data[0])\n",
    "plt.title(\"Histogram of token amount in sentence\")\n",
    "plt.xlabel(\"Amount of tokens\")\n",
    "plt.ylabel(\"Amount of sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpiUUssLo1zv"
   },
   "source": [
    "# Convert Russian tokens in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:18:01.957914Z",
     "start_time": "2019-04-08T06:17:57.645839Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jf6_fe8yo1zx"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(BASE_PATH, \"embeddings/skipgram_fasttext/araneum_none_fasttextskipgram_300_5_2018.model\")\n",
    "model = FastText.load(path)\n",
    "ru_embedder = FastTextWrapper(model)\n",
    "del path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:18:03.599724Z",
     "start_time": "2019-04-08T06:18:03.596447Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l1K4QNIEo1zz"
   },
   "outputs": [],
   "source": [
    "ru_tokenizer = tokenizer_factory(WORD_PUNCT_TOKENIZER_WITHOUT_SOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:18:05.053107Z",
     "start_time": "2019-04-08T06:18:04.856215Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oO8G0qE3o1z2"
   },
   "outputs": [],
   "source": [
    "russian_tokens = corpus_df.apply(lambda x: ru_tokenizer(x[RU_LABEL]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHrbZigyo1z8"
   },
   "source": [
    "# Wrap into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:18:07.696987Z",
     "start_time": "2019-04-08T06:18:07.668521Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lRrPA08zo1z-"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(russian_tokens, en_sentence_list)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:13.046746Z",
     "start_time": "2019-04-08T06:20:13.043795Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FEPDiFmLo10A"
   },
   "outputs": [],
   "source": [
    "train_data_set = RUENDataset(X_train, Y_train, ru_embedder, en_encoder, device=device)\n",
    "train_dataloader = DataLoader(train_data_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:14.024504Z",
     "start_time": "2019-04-08T06:20:14.021378Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MxrfBBoco10C"
   },
   "outputs": [],
   "source": [
    "test_data_set = RUENDataset(X_test, Y_test, ru_embedder, en_encoder, device=device)\n",
    "test_dataloader = DataLoader(test_data_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aR1rMHBwo10H"
   },
   "outputs": [],
   "source": [
    "del corpus_df\n",
    "del russian_tokens\n",
    "del en_sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbWNMuSqo10K"
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:19.506620Z",
     "start_time": "2019-04-08T06:20:19.500869Z"
    },
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "WxzmhRqfo10K"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_vector_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(batch_first=True, input_size=input_size, hidden_size=hidden_vector_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape [1, seq_len, fast_text_vect]\n",
    "        return: output.shape[1,hidden_size]\n",
    "                hidden_states - typle\n",
    "        \"\"\"\n",
    "        output, hidden_states = self.encoder(X)\n",
    "        output = output[0][-1]\n",
    "        return output, hidden_states\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_vector_size, vocabular_size):\n",
    "        super().__init__()\n",
    "        self.dence_in = nn.Linear(vocabular_size, input_size)\n",
    "        self.decoder = nn.LSTM(batch_first=True, input_size=input_size, hidden_size=hidden_vector_size)\n",
    "        self.dence_out = nn.Linear(hidden_vector_size, vocabular_size)\n",
    "        self.log_soft_max = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, hidden_state):\n",
    "        \"\"\"\n",
    "        Return: X.shape (1,1,vocabular_size)\n",
    "        \"\"\"\n",
    "        X = self.dence_in(X)\n",
    "        X = torch.tanh(X)\n",
    "        X, hidden_state = self.decoder(X, hidden_state)\n",
    "        X = self.dence_out(X)\n",
    "#         X = torch.sigmoid(X)\n",
    "        X = self.log_soft_max(X)\n",
    "        return X, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:22.747978Z",
     "start_time": "2019-04-08T06:20:22.730151Z"
    },
    "code_folding": [
     2
    ],
    "colab": {},
    "colab_type": "code",
    "id": "rZebHeLZo10N"
   },
   "outputs": [],
   "source": [
    "LOSS_VAL = \"LossVal\"\n",
    "BLEU_SCORE = \"BLEU\"\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 log_writer,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 encoder_optimizer,\n",
    "                 decoder_optimizer,\n",
    "                 loss,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 EOS,\n",
    "                 SOS,\n",
    "                 epoch,\n",
    "                 device,\n",
    "                 verbose=False):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_optimizer = encoder_optimizer\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.loss = loss\n",
    "        self.log_writer = log_writer\n",
    "        self.EOS = EOS\n",
    "        self.SOS = SOS\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, dataloader, validation_dataloader):\n",
    "        dataloader = self._wrap_dataloader(dataloader)\n",
    "\n",
    "        for current_epoch in range(1, self.epoch + 1):\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {current_epoch}\")\n",
    "            metric_dict = {LOSS_VAL: 0}\n",
    "            for batch in dataloader:\n",
    "                # ru_vector shape [1, seq_len_1, fast_text_vect]\n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "                # eng_vector shape [1, seq_len_2, vocab_size]\n",
    "                eng_vector = batch[EN_DS_LABEL]\n",
    "                temp_metrics = self.process_one_pair(ru_vector, eng_vector)\n",
    "                metric_dict[LOSS_VAL] += temp_metrics[LOSS_VAL]\n",
    "\n",
    "            for key, val in metric_dict.items():\n",
    "                self.log_writer.add_scalar(f\"train/{key}\", val, current_epoch)\n",
    "\n",
    "            temp_metrics = self.validate(validation_dataloader)\n",
    "\n",
    "            for key, val in temp_metrics.items():\n",
    "                self.log_writer.add_scalar(f\"validation/{key}\", val,\n",
    "                                           current_epoch)\n",
    "\n",
    "    def process_one_pair(self, ru_vector, eng_vector):\n",
    "        \"\"\"\n",
    "        ru_vector shape [1, seq_len_1, fast_text_vect]\n",
    "        eng_vector shape [1, seq_len_2, vocab_size]\n",
    "        \"\"\"\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        temp_metrics = {}\n",
    "\n",
    "        X, hidden_state = self.encoder(ru_vector)\n",
    "\n",
    "        loss_val = 0\n",
    "        loss_torch = None\n",
    "\n",
    "        Y = self.SOS\n",
    "\n",
    "        for i in range(1, eng_vector.shape[1]):\n",
    "            token = eng_vector[0][i]\n",
    "            token = token.view(1, -1, token.size()[0])\n",
    "            class_index = torch.argmax(token, dim=-1)\n",
    "\n",
    "            Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "\n",
    "            temp_loss = self.loss(Y[0], class_index[0])\n",
    "            if loss_torch is None:\n",
    "                loss_torch = temp_loss\n",
    "            else:\n",
    "                loss_torch += temp_loss\n",
    "            loss_val += temp_loss.item()\n",
    "\n",
    "            Y, word_index = self._get_pred_vect(Y)\n",
    "            if Y is None:\n",
    "                break\n",
    "            else:\n",
    "                Y = token\n",
    "\n",
    "        temp_metrics[LOSS_VAL] = loss_val\n",
    "        loss_torch.backward()\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return temp_metrics\n",
    "\n",
    "    def validate(self, dataloader):\n",
    "        bleu = 0\n",
    "        loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                # ru_vector shape [1, seq_len_1, fast_text_vect]\n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "                # eng_vector shape [1, seq_len_2, vocab_size]\n",
    "                eng_vector = batch[EN_DS_LABEL]\n",
    "\n",
    "                X, hidden_state = self.encoder(ru_vector)\n",
    "\n",
    "                loss_torch = None\n",
    "\n",
    "                Y = self.SOS\n",
    "\n",
    "                sentence = []\n",
    "                target_sentence_list = []\n",
    "                for i in range(1, eng_vector.shape[1]):\n",
    "                    token = eng_vector[0][i]\n",
    "                    token = token.view(1, -1, token.size()[0])\n",
    "                    class_index = torch.argmax(token, dim=-1)\n",
    "\n",
    "                    target_sentence_list.append(class_index[0].item())\n",
    "\n",
    "                    Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "\n",
    "                    temp_loss = self.loss(Y[0], class_index[0])\n",
    "                    if loss_torch is None:\n",
    "                        loss_torch = temp_loss\n",
    "                    else:\n",
    "                        loss_torch += temp_loss\n",
    "                    loss_val += temp_loss.item()\n",
    "\n",
    "                    Y, word_index = self._get_pred_vect(Y)\n",
    "                    sentence.append(word_index)\n",
    "                    if Y is None:\n",
    "                        break\n",
    "                bleu += sentence_bleu([sentence], target_sentence_list)\n",
    "            bleu = bleu / len(dataloader.dataset)\n",
    "\n",
    "        return {LOSS_VAL: loss_val, BLEU_SCORE: bleu}\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        dataloader = self._wrap_dataloader(dataloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = []\n",
    "            for batch in dataloader:\n",
    "                sentence = []\n",
    "                result.append(sentence)\n",
    "\n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "\n",
    "                X, hidden_state = self.encoder(ru_vector)\n",
    "                Y = self.SOS\n",
    "                for i in range(1, ru_vector.shape[1]):\n",
    "                    Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "\n",
    "                    Y, word_index = self._get_pred_vect(Y)\n",
    "                    sentence.append(word_index)\n",
    "                    if Y is None:\n",
    "                        break\n",
    "            return result\n",
    "\n",
    "    def _wrap_dataloader(self, dataloader):\n",
    "        if self.verbose:\n",
    "            dataloader = tqdm(dataloader)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_pred_vect(self, Y):\n",
    "        \"\"\"\n",
    "        Y.shape (1,1,vocabular_size)\n",
    "        \"\"\"\n",
    "        res = None\n",
    "        _, word_index = Y.topk(1)\n",
    "        word_index = word_index.item()\n",
    "        if word_index == self.EOS:\n",
    "            if self.verbose:\n",
    "                print(\"Achived EOS\")\n",
    "        else:\n",
    "            res = torch.zeros((1, 1, Y.size()[2]), device=self.device)\n",
    "            res[0, 0, word_index] = 1\n",
    "        return res, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:25.783774Z",
     "start_time": "2019-04-08T06:20:25.541982Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wwWxiZ6No10Q"
   },
   "outputs": [],
   "source": [
    "input_size = model.vector_size\n",
    "vocabular_input_size = english_vocab.max_index + 1\n",
    "hidden_size = 300\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, hidden_size, vocabular_input_size).to(device)\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:26.737718Z",
     "start_time": "2019-04-08T06:20:26.678452Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QNYwUbLHo10T"
   },
   "outputs": [],
   "source": [
    "EOS_vector = [[EOS_LABEL]]\n",
    "EOS_vector = english_vocab.transform(EOS_vector)\n",
    "EOS_vector = EOS_vector[0][0]\n",
    "\n",
    "SOS_vector = [[SOS_LABEL]]\n",
    "SOS_vector = english_vocab.transform(SOS_vector)\n",
    "SOS_vector = en_encoder.transform(SOS_vector)\n",
    "SOS_vector = torch.tensor(SOS_vector, dtype=torch.float32, device=device)\n",
    "SOS_vector = SOS_vector.view(1, -1, SOS_vector.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T06:20:28.829848Z",
     "start_time": "2019-04-08T06:20:28.826995Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "b2BLdK_Uo10V"
   },
   "outputs": [],
   "source": [
    "experiment_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "872KB3Uao10Y"
   },
   "outputs": [],
   "source": [
    "experiment_number += 1\n",
    "# log_path = os.path.join(TENSORBOARD_LOG, f\"test_{experiment_number}\")\n",
    "log_path = os.path.join(TENSORBOARD_LOG, f\"test_tanh\")\n",
    "\n",
    "\n",
    "log_writer = SummaryWriter(log_path)\n",
    "\n",
    "trainer = Trainer(log_writer=log_writer,\n",
    "                  encoder=encoder,\n",
    "                  decoder=decoder, \n",
    "                  encoder_optimizer=encoder_optimizer,\n",
    "                  decoder_optimizer=decoder_optimizer, \n",
    "                  loss=loss_function,\n",
    "                  input_size=input_size,\n",
    "                  hidden_size=hidden_size,\n",
    "                  EOS=EOS_vector,\n",
    "                  SOS=SOS_vector,\n",
    "                  epoch=5,\n",
    "                  device=device,\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wldt1lf0o10d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.train(train_dataloader, test_dataloader)\n",
    "except Exception as e:\n",
    "    traceback.print_tb(e)\n",
    "finally:\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_1uOclyo10f"
   },
   "outputs": [],
   "source": [
    "prediction = trainer.predict(test_dataloader)\n",
    "predicted_sentences_list = english_vocab.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bho5kdjYo10k"
   },
   "outputs": [],
   "source": [
    "show_translation(X_test, predicted_sentences_list)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Main.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
