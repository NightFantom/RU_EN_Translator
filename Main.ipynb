{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_LOG = os.path.join(BASE_PATH, \"tensorboard_log\")\n",
    "\n",
    "if not os.path.exists(TENSORBOARD_LOG):\n",
    "    os.makedirs(TENSORBOARD_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_LABEL = \"RU\"\n",
    "EN_LABEL = \"EN\"\n",
    "\n",
    "RU_DS_LABEL = \"RU\"\n",
    "EN_DS_LABEL = \"EN\"\n",
    "\n",
    "EOS_LABEL = \"<EOS>\"\n",
    "SOS_LABEL = \"<SOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Chosen {device.type}:{device.index} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_tensorboard_log():\n",
    "    import shutil\n",
    "    shutil.rmtree(TENSORBOARD_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(word_dir:str)-> pd.DataFrame:\n",
    "    \n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.en\")\n",
    "    data_en = load_corpus(path)\n",
    "\n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.ru\")\n",
    "    data_ru = load_corpus(path)\n",
    "    \n",
    "    df = pd.DataFrame({RU_LABEL: data_ru, EN_LABEL: data_en})\n",
    "    return df\n",
    "\n",
    "def load_corpus(path:str)->list:\n",
    "    with open(path, mode=\"r\") as file:\n",
    "        data = file.readlines()\n",
    "    data = [s.strip().lower() for s in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPunctTokenizerWrapper:\n",
    "    \n",
    "    def __init__(self, with_SOS=False):\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.with_SOS = with_SOS\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        token_list = self.tokenizer.tokenize(text)\n",
    "        if self.with_SOS:\n",
    "            token_list.insert(0, SOS_LABEL)\n",
    "        token_list.append(EOS_LABEL)\n",
    "        return token_list\n",
    "\n",
    "def tokenizer_factory(factory_name):\n",
    "    tokenizer = None\n",
    "    if factory_name == \"wpt\":\n",
    "        tokenizer = WordPunctTokenizerWrapper(with_SOS=True)\n",
    "    elif factory_name == \"ru_tok\":\n",
    "        tokenizer = WordPunctTokenizerWrapper()\n",
    "    return tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocabular:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.max_index = -1\n",
    "        \n",
    "    def fit(self, X, Y=None):\n",
    "        for sample in X:\n",
    "            for feature in sample:\n",
    "                if feature not in self.word_to_index:\n",
    "                    self.max_index += 1\n",
    "                    self.word_to_index[feature] = self.max_index\n",
    "                    \n",
    "        for word, index in self.word_to_index.items():\n",
    "            self.index_to_word[index] = word\n",
    "            \n",
    "    def transform(self, X):\n",
    "        sentense_list = []\n",
    "        for sample in X:\n",
    "            sentence = []\n",
    "            sentense_list.append(sentence)\n",
    "            for feature in sample:\n",
    "                index = self.word_to_index[feature]\n",
    "                sentence.append(index)\n",
    "        return sentense_list\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        sentense_list = []\n",
    "        for sample in X:\n",
    "            sentence = []\n",
    "            sentense_list.append(sentence)\n",
    "            for feature in sample:\n",
    "                word = self.index_to_word[feature]\n",
    "                sentence.append(word)\n",
    "        return sentense_list\n",
    "    \n",
    "    \n",
    "class FastTextWrapper:\n",
    "    \n",
    "    def __init__(self, embedder):\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def transform(self, data):\n",
    "        res = []\n",
    "        for sentence in data:\n",
    "            temp = []\n",
    "            for x in sentence:\n",
    "                if x in self.embedder.wv:\n",
    "                    temp.append(x)\n",
    "            vector_np = self.embedder.wv[temp]\n",
    "            res.append(vector_np)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RUENDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ru_data_list, en_data_list, ru_encoder, en_encoder, device):\n",
    "        self.ru_data = ru_data_list\n",
    "        self.en_data = en_data_list\n",
    "        self.en_encoder = en_encoder\n",
    "        self.ru_encoder = ru_encoder\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.en_data)\n",
    "    \n",
    "    def __getitem__(self, pos):\n",
    "        en_vector = self._get_en_sentence(pos)\n",
    "        en_vector = torch.tensor(en_vector, dtype=torch.float32, device=device)\n",
    "        ru_vector = self._get_ru_sentence(pos)\n",
    "        ru_vector = torch.tensor(ru_vector, dtype=torch.float32, device=device)\n",
    "        \n",
    "        return {RU_DS_LABEL:ru_vector, EN_DS_LABEL:en_vector}\n",
    "    \n",
    "    def _get_ru_sentence(self, pos):\n",
    "        ru_sentence_list = self.ru_data[pos]\n",
    "        ru_sentence_list = [ru_sentence_list]\n",
    "        vector = self.ru_encoder.transform(ru_sentence_list)\n",
    "        vector = vector[0]\n",
    "        return vector\n",
    "    \n",
    "    def _get_en_sentence(self, pos):\n",
    "        en_sentence_list = self.en_data[pos]\n",
    "        sentence_list = self.en_encoder.transform(en_sentence_list)\n",
    "        return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_translation(ru_list, en_list):\n",
    "    index = random.randint(0, len(ru_list)-1)\n",
    "    ru_sent = \" \".join(ru_list[index])\n",
    "    en_sent = \" \".join(en_list[index])\n",
    "    print(ru_sent)\n",
    "    print(en_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = load_data(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = corpus_df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert English tokens in one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_factory(\"wpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = Vocabular()\n",
    "english_tokens = corpus_df.apply(lambda x: tokenizer(x[EN_LABEL]), axis=1)\n",
    "english_vocab.fit(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encoder = LabelBinarizer(sparse_output=False)\n",
    "en_encoder.fit(range(english_vocab.max_index+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence_list = english_vocab.transform(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab.max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenth = np.max([len(x) for x in en_sentence_list])\n",
    "max_lenth += 1\n",
    "print(f\"Max sequence lenth is {max_lenth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Length histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_data = np.histogram( [len(x) for x in sentence_list], bins=max_lenth)\n",
    "\n",
    "plt.bar(range(max_lenth), hist_data[0])\n",
    "plt.title(\"Histogram of token amount in sentence\")\n",
    "plt.xlabel(\"Amount of tokens\")\n",
    "plt.ylabel(\"Amount of sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Russian tokens in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(BASE_PATH, \"embeddings/skipgram_fasttext/araneum_none_fasttextskipgram_300_5_2018.model\")\n",
    "model = FastText.load(path)\n",
    "ru_embedder = FastTextWrapper(model)\n",
    "del path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_tokenizer = tokenizer_factory(\"ru_tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_tokens = corpus_df.apply(lambda x: ru_tokenizer(x[RU_LABEL]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(russian_tokens, en_sentence_list)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = RUENDataset(X_train, Y_train, ru_embedder, en_encoder, device=device)\n",
    "train_dataloader = DataLoader(train_data_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set = RUENDataset(X_test, Y_test, ru_embedder, en_encoder, device=device)\n",
    "test_dataloader = DataLoader(test_data_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus_df\n",
    "del russian_tokens\n",
    "del en_sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_vector_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(batch_first=True, input_size=input_size, hidden_size=hidden_vector_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape [1, seq_len, fast_text_vect]\n",
    "        return: output.shape[1,hidden_size]\n",
    "                hidden_states - typle\n",
    "        \"\"\"\n",
    "        output, hidden_states = self.encoder(X)\n",
    "        output = output[0][-1]\n",
    "        return output, hidden_states\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_vector_size, vocabular_size):\n",
    "        super().__init__()\n",
    "        self.dence_in = nn.Linear(vocabular_size, input_size)\n",
    "        self.decoder = nn.LSTM(batch_first=True, input_size=input_size, hidden_size=hidden_vector_size)\n",
    "        self.dence_out = nn.Linear(hidden_vector_size, vocabular_size)\n",
    "        self.log_soft_max = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, hidden_state):\n",
    "        \"\"\"\n",
    "        Return: X.shape (1,1,vocabular_size)\n",
    "        \"\"\"\n",
    "        X = self.dence_in(X)\n",
    "        X = torch.tanh(X)\n",
    "        X, hidden_state = self.decoder(X, hidden_state)\n",
    "        X = self.dence_out(X)\n",
    "#         X = torch.sigmoid(X)\n",
    "        X = self.log_soft_max(X)\n",
    "        return X, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "LOSS_VAL = \"LossVal\"\n",
    "BLEU_SCORE = \"BLEU\"\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 log_writer,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 encoder_optimizer,\n",
    "                 decoder_optimizer,\n",
    "                 loss,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 EOS,\n",
    "                 SOS,\n",
    "                 epoch,\n",
    "                 device,\n",
    "                 verbose=False):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_optimizer = encoder_optimizer\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.loss = loss\n",
    "        self.log_writer = log_writer\n",
    "        self.EOS = EOS\n",
    "        self.SOS = SOS\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, dataloader, validation_dataloader):\n",
    "        dataloader = self._wrap_dataloader(dataloader)\n",
    "\n",
    "        for current_epoch in range(1, self.epoch + 1):\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {current_epoch}\")\n",
    "            metric_dict = {LOSS_VAL: 0}\n",
    "            for batch in dataloader:\n",
    "                # ru_vector shape [1, seq_len_1, fast_text_vect]\n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "                # eng_vector shape [1, seq_len_2, vocab_size]\n",
    "                eng_vector = batch[EN_DS_LABEL]\n",
    "                temp_metrics = self.process_one_pair(ru_vector, eng_vector)\n",
    "                metric_dict[LOSS_VAL] += temp_metrics[LOSS_VAL]\n",
    "\n",
    "            for key, val in metric_dict.items():\n",
    "                self.log_writer.add_scalar(f\"train/{key}\", val, current_epoch)\n",
    "\n",
    "            temp_metrics = self.validate(validation_dataloader)\n",
    "\n",
    "            for key, val in temp_metrics.items():\n",
    "                self.log_writer.add_scalar(f\"validation/{key}\", val,\n",
    "                                           current_epoch)\n",
    "\n",
    "    def process_one_pair(self, ru_vector, eng_vector):\n",
    "        \"\"\"\n",
    "        ru_vector shape [1, seq_len_1, fast_text_vect]\n",
    "        eng_vector shape [1, seq_len_2, vocab_size]\n",
    "        \"\"\"\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        temp_metrics = {}\n",
    "\n",
    "        X, hidden_state = self.encoder(ru_vector)\n",
    "\n",
    "        loss_val = 0\n",
    "        loss_torch = None\n",
    "\n",
    "        Y = self.SOS\n",
    "\n",
    "        for i in range(1, eng_vector.shape[1]):\n",
    "            token = eng_vector[0][i]\n",
    "            token = token.view(1, -1, token.size()[0])\n",
    "            class_index = torch.argmax(token, dim=-1)\n",
    "\n",
    "            Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "\n",
    "            temp_loss = self.loss(Y[0], class_index[0])\n",
    "            if loss_torch is None:\n",
    "                loss_torch = temp_loss\n",
    "            else:\n",
    "                loss_torch += temp_loss\n",
    "            loss_val += temp_loss.item()\n",
    "\n",
    "            Y, word_index = self._get_pred_vect(Y)\n",
    "            if Y is None:\n",
    "                break\n",
    "            else:\n",
    "                Y = token\n",
    "\n",
    "        temp_metrics[LOSS_VAL] = loss_val\n",
    "        loss_torch.backward()\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return temp_metrics\n",
    "\n",
    "    def validate(self, dataloader):\n",
    "        bleu = 0\n",
    "        loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                # ru_vector shape [1, seq_len_1, fast_text_vect]\n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "                # eng_vector shape [1, seq_len_2, vocab_size]\n",
    "                eng_vector = batch[EN_DS_LABEL]\n",
    "\n",
    "                X, hidden_state = self.encoder(ru_vector)\n",
    "\n",
    "                loss_torch = None\n",
    "\n",
    "                Y = self.SOS\n",
    "\n",
    "                sentence = []\n",
    "                target_sentence_list = []\n",
    "                for i in range(1, eng_vector.shape[1]):\n",
    "                    token = eng_vector[0][i]\n",
    "                    token = token.view(1, -1, token.size()[0])\n",
    "                    class_index = torch.argmax(token, dim=-1)\n",
    "\n",
    "                    target_sentence_list.append(class_index[0].item())\n",
    "\n",
    "                    Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "\n",
    "                    temp_loss = self.loss(Y[0], class_index[0])\n",
    "                    if loss_torch is None:\n",
    "                        loss_torch = temp_loss\n",
    "                    else:\n",
    "                        loss_torch += temp_loss\n",
    "                    loss_val += temp_loss.item()\n",
    "\n",
    "                    Y, word_index = self._get_pred_vect(Y)\n",
    "                    sentence.append(word_index)\n",
    "                    if Y is None:\n",
    "                        break\n",
    "                bleu += sentence_bleu([sentence], target_sentence_list)\n",
    "            bleu = bleu / len(dataloader.dataset)\n",
    "\n",
    "        return {LOSS_VAL: loss_val, BLEU_SCORE: bleu}\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        dataloader = self._wrap_dataloader(dataloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = []\n",
    "            for batch in dataloader:\n",
    "                sentence = []\n",
    "                result.append(sentence)\n",
    "\n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "\n",
    "                X, hidden_state = self.encoder(ru_vector)\n",
    "                Y = self.SOS\n",
    "                for i in range(1, ru_vector.shape[1]):\n",
    "                    Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "\n",
    "                    Y, word_index = self._get_pred_vect(Y)\n",
    "                    sentence.append(word_index)\n",
    "                    if Y is None:\n",
    "                        break\n",
    "            return result\n",
    "\n",
    "    def _wrap_dataloader(self, dataloader):\n",
    "        if self.verbose:\n",
    "            dataloader = tqdm(dataloader)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_pred_vect(self, Y):\n",
    "        \"\"\"\n",
    "        Y.shape (1,1,vocabular_size)\n",
    "        \"\"\"\n",
    "        res = None\n",
    "        _, word_index = Y.topk(1)\n",
    "        word_index = word_index.item()\n",
    "        if word_index == self.EOS:\n",
    "            if self.verbose:\n",
    "                print(\"Achived EOS\")\n",
    "        else:\n",
    "            res = torch.zeros((1, 1, Y.size()[2]), device=self.device)\n",
    "            res[0, 0, word_index] = 1\n",
    "        return res, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = model.vector_size\n",
    "vocabular_input_size = english_vocab.max_index + 1\n",
    "hidden_size = 300\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, hidden_size, vocabular_input_size).to(device)\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_vector = [[EOS_LABEL]]\n",
    "EOS_vector = english_vocab.transform(EOS_vector)\n",
    "EOS_vector = EOS_vector[0][0]\n",
    "\n",
    "SOS_vector = [[SOS_LABEL]]\n",
    "SOS_vector = english_vocab.transform(SOS_vector)\n",
    "SOS_vector = en_encoder.transform(SOS_vector)\n",
    "SOS_vector = torch.tensor(SOS_vector, dtype=torch.float32, device=device)\n",
    "SOS_vector = SOS_vector.view(1, -1, SOS_vector.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "experiment_number += 1\n",
    "# log_path = os.path.join(TENSORBOARD_LOG, f\"test_{experiment_number}\")\n",
    "log_path = os.path.join(TENSORBOARD_LOG, f\"test_tanh\")\n",
    "\n",
    "\n",
    "log_writer = SummaryWriter(log_path)\n",
    "\n",
    "trainer = Trainer(log_writer=log_writer,\n",
    "                  encoder=encoder,\n",
    "                  decoder=decoder, \n",
    "                  encoder_optimizer=encoder_optimizer,\n",
    "                  decoder_optimizer=decoder_optimizer, \n",
    "                  loss=loss_function,\n",
    "                  input_size=input_size,\n",
    "                  hidden_size=hidden_size,\n",
    "                  EOS=EOS_vector,\n",
    "                  SOS=SOS_vector,\n",
    "                  epoch=5,\n",
    "                  device=device,\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.train(train_dataloader, test_dataloader)\n",
    "except Exception as e:\n",
    "    traceback.print_tb(e)\n",
    "finally:\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = trainer.predict(test_dataloader)\n",
    "predicted_sentences_list = english_vocab.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_translation(X_test, predicted_sentences_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:denis_u_env]",
   "language": "python",
   "name": "conda-env-denis_u_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "598.85px",
    "left": "1488px",
    "right": "20px",
    "top": "120px",
    "width": "345px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
