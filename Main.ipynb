{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:04:52.287967Z",
     "start_time": "2019-01-08T07:04:52.285358Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:08:35.875376Z",
     "start_time": "2019-01-08T10:08:35.869835Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "# from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:05:03.915261Z",
     "start_time": "2019-01-08T07:05:03.912808Z"
    }
   },
   "outputs": [],
   "source": [
    "RU_LABEL = \"RU\"\n",
    "EN_LABEL = \"EN\"\n",
    "\n",
    "RU_DS_LABEL = \"RU\"\n",
    "EN_DS_LABEL = \"EN\"\n",
    "\n",
    "EOS_LABEL = \"<EOS>\"\n",
    "SOS_LABEL = \"<SOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:05:04.018819Z",
     "start_time": "2019-01-08T07:05:03.921274Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(word_dir:str)-> pd.DataFrame:\n",
    "    \n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.en\")\n",
    "    data_en = load_corpus(path)\n",
    "\n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.ru\")\n",
    "    data_ru = load_corpus(path)\n",
    "    \n",
    "    df = pd.DataFrame({RU_LABEL: data_ru, EN_LABEL: data_en})\n",
    "    return df\n",
    "\n",
    "def load_corpus(path:str)->list:\n",
    "    with open(path, mode=\"r\") as file:\n",
    "        data = file.readlines()\n",
    "    data = [s.strip().lower() for s in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:05:04.175523Z",
     "start_time": "2019-01-08T07:05:04.028218Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordPunctTokenizerWrapper:\n",
    "    \n",
    "    def __init__(self, with_SOS=False):\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.with_SOS = with_SOS\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        token_list = self.tokenizer.tokenize(text)\n",
    "        if self.with_SOS:\n",
    "            token_list.insert(0, SOS_LABEL)\n",
    "        token_list.append(EOS_LABEL)\n",
    "        return token_list\n",
    "\n",
    "def tokenizer_factory(factory_name):\n",
    "    tokenizer = None\n",
    "    if factory_name == \"wpt\":\n",
    "        tokenizer = WordPunctTokenizerWrapper(with_SOS=True)\n",
    "    elif factory_name == \"ru_tok\":\n",
    "        tokenizer = WordPunctTokenizerWrapper()\n",
    "    return tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:05:04.259591Z",
     "start_time": "2019-01-08T07:05:04.181291Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocabular:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.max_index = -1\n",
    "        \n",
    "    def fit(self, X, Y=None):\n",
    "        for sample in X:\n",
    "            for feature in sample:\n",
    "                if feature not in self.word_to_index:\n",
    "                    self.max_index += 1\n",
    "                    self.word_to_index[feature] = self.max_index\n",
    "                    \n",
    "        for word, index in self.word_to_index.items():\n",
    "            self.index_to_word[index] = word\n",
    "            \n",
    "    def transform(self, X):\n",
    "        sentense_list = []\n",
    "        for sample in X:\n",
    "            sentence = []\n",
    "            sentense_list.append(sentence)\n",
    "            for feature in sample:\n",
    "                index = self.word_to_index[feature]\n",
    "                sentence.append(index)\n",
    "        return sentense_list\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        sentense_list = []\n",
    "        for sample in X:\n",
    "            sentence = []\n",
    "            sentense_list.append(sentence)\n",
    "            for feature in sample:\n",
    "                word = self.index_to_word[feature]\n",
    "                sentence.append(word)\n",
    "        return sentense_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T08:42:55.172872Z",
     "start_time": "2019-01-07T08:42:55.168256Z"
    }
   },
   "outputs": [],
   "source": [
    "def allignment(sentence_list, encoder, max_length):\n",
    "    result = []\n",
    "    for sentence in tqdm_notebook(sentence_list):\n",
    "        temp = encoder.transform(sentence)\n",
    "#         size = temp.shape[0]\n",
    "#         if size >= max_length:\n",
    "#             temp = temp[:max_length]\n",
    "#         zero_vector_np = sparse.csr_matrix((max_length - temp.shape[0], temp.shape[1]), dtype=np.int8)\n",
    "# #         temp = sparse.csr_matrix(temp, dtype=np.int8)\n",
    "#         temp = sparse.vstack([temp, zero_vector_np], dtype=np.int8)\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:05:08.024080Z",
     "start_time": "2019-01-08T07:05:08.019864Z"
    }
   },
   "outputs": [],
   "source": [
    "class FastTextWrapper:\n",
    "    \n",
    "    def __init__(self, embedder):\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def transform(self, data):\n",
    "        res = []\n",
    "        for sentence in data:\n",
    "            temp = []\n",
    "            for x in sentence:\n",
    "                if x in self.embedder.wv:\n",
    "                    temp.append(x)\n",
    "            vector_np = self.embedder.wv[temp]\n",
    "            res.append(vector_np)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T08:01:11.115715Z",
     "start_time": "2019-01-08T08:01:11.108071Z"
    }
   },
   "outputs": [],
   "source": [
    "class RUENDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ru_data_list, en_data_list, ru_encoder, en_encoder):\n",
    "        self.ru_data = ru_data_list\n",
    "        self.en_data = en_data_list\n",
    "        self.en_encoder = en_encoder\n",
    "        self.ru_encoder = ru_encoder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.en_data)\n",
    "    \n",
    "    def __getitem__(self, pos):\n",
    "        en_vector = self._get_en_sentence(pos)\n",
    "        en_vector = torch.tensor(en_vector, dtype=torch.float32)\n",
    "        ru_vector = self._get_ru_sentence(pos)\n",
    "        ru_vector = torch.tensor(ru_vector, dtype=torch.float32)\n",
    "        \n",
    "        return {RU_DS_LABEL:ru_vector, EN_DS_LABEL:en_vector}\n",
    "    \n",
    "    def _get_ru_sentence(self, pos):\n",
    "        ru_sentence_list = self.ru_data[pos]\n",
    "        ru_sentence_list = [ru_sentence_list]\n",
    "        vector = self.ru_encoder.transform(ru_sentence_list)\n",
    "        vector = vector[0]\n",
    "        return vector\n",
    "    \n",
    "    def _get_en_sentence(self, pos):\n",
    "        en_sentence_list = self.en_data[pos]\n",
    "        sentence_list = self.en_encoder.transform(en_sentence_list)\n",
    "        return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:10:45.216003Z",
     "start_time": "2019-01-08T10:10:45.211892Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_translation(ru_list, en_list):\n",
    "    index = random.randint(0, len(ru_list)-1)\n",
    "    ru_sent = \" \".join(ru_list[index])\n",
    "    en_sent = \" \".join(en_list[index])\n",
    "    print(ru_sent)\n",
    "    print(en_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:05:17.549919Z",
     "start_time": "2019-01-08T07:05:13.097432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_df = load_data(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:03:28.162401Z",
     "start_time": "2019-01-04T17:03:28.156369Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:05:26.822441Z",
     "start_time": "2019-01-08T07:05:26.819168Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_df = corpus_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:26:32.248380Z",
     "start_time": "2019-01-04T17:26:32.242854Z"
    }
   },
   "source": [
    "# Convert English tokens in one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:27:15.262158Z",
     "start_time": "2019-01-08T07:27:15.257315Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_factory(\"wpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:27:15.466614Z",
     "start_time": "2019-01-08T07:27:15.461933Z"
    }
   },
   "outputs": [],
   "source": [
    "english_vocab = Vocabular()\n",
    "english_tokens = corpus_df.apply(lambda x: tokenizer(x[EN_LABEL]), axis=1)\n",
    "english_vocab.fit(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:27:15.699349Z",
     "start_time": "2019-01-08T07:27:15.694112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_encoder = LabelBinarizer(sparse_output=False)\n",
    "en_encoder.fit(range(english_vocab.max_index+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:27:18.877643Z",
     "start_time": "2019-01-08T07:27:18.872825Z"
    }
   },
   "outputs": [],
   "source": [
    "en_sentence_list = english_vocab.transform(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T08:54:36.932776Z",
     "start_time": "2019-01-06T08:54:36.810773Z"
    }
   },
   "outputs": [],
   "source": [
    "max_lenth = np.max([len(x) for x in en_sentence_list])\n",
    "max_lenth += 1\n",
    "print(f\"Max sequence lenth is {max_lenth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:50:34.335143Z",
     "start_time": "2019-01-06T08:59:58.703454Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectors = allignment(en_sentence_list, en_encoder, max_lenth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:50:42.920576Z",
     "start_time": "2019-01-06T12:50:42.916235Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T12:53:02.471316Z",
     "start_time": "2019-01-06T12:50:51.133409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = os.path.join(BASE_PATH, \"dump/english_ohe.pkl\")\n",
    "with open(path, mode=\"wb\") as file:\n",
    "    pickle.dump(vectors, file)\n",
    "    \n",
    "del path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T07:32:33.085420Z",
     "start_time": "2019-01-05T07:32:33.081733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = os.path.join(BASE_PATH, \"dump/english_ohe.pkl\")\n",
    "with open(path, mode=\"rb\") as file:\n",
    "    vectors = pickle.load(file)\n",
    "\n",
    "del path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Length histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:21:27.931963Z",
     "start_time": "2019-01-04T21:21:27.653196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_data = np.histogram( [len(x) for x in sentence_list], bins=max_lenth)\n",
    "\n",
    "plt.bar(range(max_lenth), hist_data[0])\n",
    "plt.title(\"Histogram of token amount in sentence\")\n",
    "plt.xlabel(\"Amount of tokens\")\n",
    "plt.ylabel(\"Amount of sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Russian tokens in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:06:03.528481Z",
     "start_time": "2019-01-08T07:05:48.543798Z"
    }
   },
   "outputs": [],
   "source": [
    "path = os.path.join(BASE_PATH, \"embeddings/skipgram_fasttext/araneum_none_fasttextskipgram_300_5_2018.model\")\n",
    "model = FastText.load(path)\n",
    "ru_embedder = FastTextWrapper(model)\n",
    "del path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:06:03.543272Z",
     "start_time": "2019-01-08T07:06:03.541071Z"
    }
   },
   "outputs": [],
   "source": [
    "ru_tokenizer = tokenizer_factory(\"ru_tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T07:06:03.634155Z",
     "start_time": "2019-01-08T07:06:03.554519Z"
    }
   },
   "outputs": [],
   "source": [
    "russian_tokens = corpus_df.apply(lambda x: ru_tokenizer(x[RU_LABEL]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T08:01:17.718793Z",
     "start_time": "2019-01-08T08:01:17.714287Z"
    }
   },
   "outputs": [],
   "source": [
    "data_set = RUENDataset(russian_tokens, en_sentence_list, ru_embedder, en_encoder)\n",
    "dataloader = DataLoader(data_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T08:04:31.848561Z",
     "start_time": "2019-01-08T08:04:31.844271Z"
    }
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataloader)\n",
    "data = next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T09:20:27.757475Z",
     "start_time": "2019-01-08T09:20:27.751058Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_vector_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(batch_first=True, input_size=input_size, hidden_size=hidden_vector_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "#         X = X.view(1,-1, X.size()[2])\n",
    "        output, hidden_states = self.encoder(X)\n",
    "        output = output[0][-1]\n",
    "        return output, hidden_states\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_vector_size):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.LSTM(batch_first=True, input_size=input_size, hidden_size=hidden_vector_size)\n",
    "        \n",
    "    def forward(self, X, hidden_state):\n",
    "        \"\"\"\n",
    "        Return: X shape (1,1,VOC_SIZE)\n",
    "        \"\"\"\n",
    "        X, hidden_state = self.decoder(X, hidden_state)\n",
    "        X = nn.functional.softmax(X,dim=2)\n",
    "        return X, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:04:19.370000Z",
     "start_time": "2019-01-08T10:04:19.353364Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model_save_path,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 encoder_optimizer,\n",
    "                 decoder_optimizer,\n",
    "                 loss,\n",
    "                 input_size, hidden_size,\n",
    "                 EOS,\n",
    "                 SOS,\n",
    "                 epoch):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_optimizer = encoder_optimizer\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.loss = loss\n",
    "        self.model_save_path = model_save_path\n",
    "        self.EOS = EOS\n",
    "        self.SOS = SOS\n",
    "        self.epoch = epoch\n",
    "\n",
    "    \n",
    "    def train(self, dataloader):\n",
    "        for i in range(self.epoch):\n",
    "            for batch in dataloader:\n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "                eng_vector = batch[EN_DS_LABEL]\n",
    "                self.process_one_pair(ru_vector, eng_vector)\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        with torch.no_grad():\n",
    "            result = []\n",
    "            for batch in dataloader:\n",
    "                sentence = []\n",
    "                result.append(sentence)\n",
    "                \n",
    "                ru_vector = batch[RU_DS_LABEL]\n",
    "                eng_vector = batch[EN_DS_LABEL]\n",
    "                \n",
    "                X, hidden_state = self.encoder(ru_vector)\n",
    "                Y = self.SOS\n",
    "                for i in range(1, ru_vector.shape[1]):\n",
    "                    Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "                    \n",
    "                    _, word_index = Y.topk(1)\n",
    "                    word_index = word_index.item()\n",
    "                    sentence.append(word_index)\n",
    "                    if word_index == self.EOS:\n",
    "                        print(\"Achived EOS\")\n",
    "                        break\n",
    "                    else:\n",
    "                        Y = torch.zeros(1,1, Y.size()[2])\n",
    "                        Y[0,0, word_index] =1\n",
    "            return result\n",
    "                \n",
    "    \n",
    "    def process_one_pair(self, ru_vector, eng_vector):\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        X, hidden_state = self.encoder(ru_vector)\n",
    "\n",
    "        loss_val = 0\n",
    "        loss_torch = None\n",
    "        \n",
    "        Y = self.SOS\n",
    "\n",
    "        for i in range(1, eng_vector.shape[1]):\n",
    "            token = eng_vector[0][i]\n",
    "            token = token.view(1, -1, token.size()[0])\n",
    "            class_index = torch.argmax(token,dim=1)\n",
    "            \n",
    "            Y, hidden_state = self.decoder(Y, hidden_state)\n",
    "                        \n",
    "            temp_loss = self.loss(Y, class_index)            \n",
    "            if loss_torch is None:\n",
    "                loss_torch = temp_loss\n",
    "            else:\n",
    "                loss_torch += temp_loss\n",
    "            loss_val += temp_loss.item()\n",
    "            \n",
    "            _, word_index = Y.topk(1)\n",
    "            word_index = word_index.item()\n",
    "            if word_index == self.EOS:\n",
    "                print(\"Achived EOS\")\n",
    "                break\n",
    "            else:\n",
    "                Y = torch.zeros(1,1, Y.size()[2])\n",
    "                Y[0,0, word_index] =1\n",
    "\n",
    "        loss_torch.backward()\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:04:20.590073Z",
     "start_time": "2019-01-08T10:04:20.583204Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = model.vector_size\n",
    "decoder_input_size = english_vocab.max_index + 1\n",
    "hidden_size = decoder_input_size\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(decoder_input_size, hidden_size)\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=0.01)\n",
    "\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:04:20.940427Z",
     "start_time": "2019-01-08T10:04:20.933492Z"
    }
   },
   "outputs": [],
   "source": [
    "EOS_vector = [[EOS_LABEL]]\n",
    "EOS_vector = english_vocab.transform(EOS_vector)\n",
    "EOS_vector = EOS_vector[0][0]\n",
    "\n",
    "SOS_vector = [[SOS_LABEL]]\n",
    "SOS_vector = english_vocab.transform(SOS_vector)\n",
    "SOS_vector = en_encoder.transform(SOS_vector)\n",
    "SOS_vector = torch.tensor(SOS_vector, dtype=torch.float32)\n",
    "SOS_vector = SOS_vector.view(1, -1, SOS_vector.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:04:21.135924Z",
     "start_time": "2019-01-08T10:04:21.131588Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "save_path = \"\"\n",
    "\n",
    "trainer = Trainer(model_save_path=save_path,\n",
    "                  encoder=encoder,\n",
    "                  decoder=decoder, \n",
    "                  encoder_optimizer=encoder_optimizer,\n",
    "                  decoder_optimizer=decoder_optimizer, \n",
    "                  loss=loss_function,\n",
    "                  input_size=input_size,\n",
    "                  hidden_size=hidden_size,\n",
    "                  EOS=EOS_vector,\n",
    "                  SOS=SOS_vector,\n",
    "                  epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:04:21.987710Z",
     "start_time": "2019-01-08T10:04:21.364770Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:04:22.728007Z",
     "start_time": "2019-01-08T10:04:22.644305Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = trainer.predict(dataloader)\n",
    "predicted_sentences_list = english_vocab.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T10:10:49.102412Z",
     "start_time": "2019-01-08T10:10:49.099055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "обычно я просыпался в 7 : 30 , спускался вниз и видел что дверь в дом открыта , на кухне и в гостиной стоит 600 банок пива и дома никого нет . <EOS>\n",
      "albums go albums human human no no human no no human no no no no no no no no no no no no no no\n"
     ]
    }
   ],
   "source": [
    "show_translation(russian_tokens, predicted_sentences_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "598.85px",
    "left": "1488px",
    "right": "20px",
    "top": "120px",
    "width": "345px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
