{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:15:24.784777Z",
     "start_time": "2019-04-20T16:15:24.776722Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:15:26.186887Z",
     "start_time": "2019-04-20T16:15:25.295134Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import translator_constants.global_constant as glc\n",
    "from tokenizer.word_punct_tokenizer import tokenizer_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:15:26.287916Z",
     "start_time": "2019-04-20T16:15:26.283307Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(word_dir: str) -> pd.DataFrame:\n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.en\")\n",
    "    data_en = load_corpus(path)\n",
    "\n",
    "    path = os.path.join(word_dir, \"data/corpus.en_ru.1m.ru\")\n",
    "    data_ru = load_corpus(path)\n",
    "\n",
    "    df = pd.DataFrame({glc.RU_LABEL: data_ru, glc.EN_LABEL: data_en})\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_corpus(path: str) -> list:\n",
    "    with open(path, mode=\"r\") as file:\n",
    "        data = file.readlines()\n",
    "    data = [s.strip().lower() for s in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:15:30.132523Z",
     "start_time": "2019-04-20T16:15:26.864930Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_df = load_data(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T15:58:43.305332Z",
     "start_time": "2019-04-20T15:58:43.295061Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:15:32.418358Z",
     "start_time": "2019-04-20T16:15:32.413453Z"
    }
   },
   "outputs": [],
   "source": [
    "def description_length(tokens_seq, percemtile = 90):\n",
    "    temp_length_list = []\n",
    "    for sentence in tokens_seq:\n",
    "        length_int = len(sentence)\n",
    "        temp_length_list.append(length_int)\n",
    "    length_np = np.array(temp_length_list)\n",
    "    \n",
    "    print(f\"Max length: {length_np.max()}\")\n",
    "    print(f\"Min length: {length_np.min()}\")\n",
    "    print(f\"Mean length: {length_np.mean()}\")\n",
    "    print(f\"Median length: {np.median(length_np)}\")\n",
    "    print(f\"{percemtile} percemtile length: {np.percentile(length_np, percemtile)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:16:15.423892Z",
     "start_time": "2019-04-20T16:15:48.235930Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_factory(glc.WORD_PUNCT_TOKENIZER_WITH_SOS)\n",
    "english_tokens = corpus_df.apply(lambda x: tokenizer(x[glc.EN_LABEL]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:16:15.954590Z",
     "start_time": "2019-04-20T16:16:15.621397Z"
    }
   },
   "outputs": [],
   "source": [
    "description_length(english_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:16:42.993743Z",
     "start_time": "2019-04-20T16:16:16.213515Z"
    }
   },
   "outputs": [],
   "source": [
    "ru_tokenizer = tokenizer_factory(glc.WORD_PUNCT_TOKENIZER_WITHOUT_SOS)\n",
    "russian_tokens = corpus_df.apply(lambda x: ru_tokenizer(x[glc.RU_LABEL]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:16:43.488630Z",
     "start_time": "2019-04-20T16:16:43.281339Z"
    }
   },
   "outputs": [],
   "source": [
    "description_length(russian_tokens, percemtile=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:16:43.790257Z",
     "start_time": "2019-04-20T16:16:43.788025Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_ENGLISH_SEQUENCE_LEN = 47\n",
    "MAX_RUSSIAN_SEQUENCE_LEN = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:18:03.913548Z",
     "start_time": "2019-04-20T16:16:44.077712Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Initial corpus size: {corpus_df.shape[0]}\")\n",
    "temp_list = []\n",
    "for i in tqdm(range(corpus_df.shape[0])):\n",
    "    prediction = len(english_tokens[i]) < MAX_ENGLISH_SEQUENCE_LEN and len(russian_tokens[i]) < MAX_RUSSIAN_SEQUENCE_LEN\n",
    "    temp_list.append(prediction)\n",
    "sub_corpus = corpus_df[temp_list]\n",
    "\n",
    "print(f\"Shrinked corpus size: {sub_corpus.shape[0]}\")\n",
    "del temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:18:45.519833Z",
     "start_time": "2019-04-20T16:18:45.512437Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_corpus.iloc[0][glc.RU_LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:19:11.850063Z",
     "start_time": "2019-04-20T16:19:11.839634Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_corpus.iloc[0][glc.EN_LABEL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T16:22:09.626783Z",
     "start_time": "2019-04-20T16:22:03.687075Z"
    }
   },
   "outputs": [],
   "source": [
    "path = os.path.join(BASE_PATH, \"data/shrinked_corpus.csv\")\n",
    "sub_corpus.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
